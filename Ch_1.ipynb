{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c341dc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-15 13:06:56.717446: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-15 13:06:56.717477: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-15 13:06:58.812343: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-01-15 13:06:58.812938: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-01-15 13:06:58.813838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-15 13:06:58.814299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:26:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5\n",
      "coreClock: 1.875GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s\n",
      "2023-01-15 13:06:58.814384: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-15 13:06:58.814464: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-01-15 13:06:58.814533: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-01-15 13:06:58.814570: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-01-15 13:06:58.814596: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-01-15 13:06:58.814620: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-01-15 13:06:58.814679: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-01-15 13:06:58.814743: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-01-15 13:06:58.814751: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc9fe474",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cf56e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "World\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Hello World!\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7525488c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World\n"
     ]
    }
   ],
   "source": [
    "token = doc[1]\n",
    "print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c58c102c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World!\n"
     ]
    }
   ],
   "source": [
    "span = doc[1:3]\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "341f4e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index        [0, 1, 2, 3, 4]\n",
      "Text:        ['It', 'costs', '$', '5', '.']\n",
      "is_alpha:    [True, True, False, False, False]\n",
      "is_punct:    [False, False, False, False, True]\n",
      "like_num:    [False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "# Lexical attributes\n",
    "doc = nlp(\"It costs $5.\")\n",
    "print(\"Index       \", [token.i for token in doc])\n",
    "print(\"Text:       \", [token.text for token in doc])\n",
    "print(\"is_alpha:   \", [token.is_alpha for token in doc])\n",
    "print(\"is_punct:   \", [token.is_punct for token in doc])\n",
    "print(\"like_num:   \", [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b046d900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index        [0, 1, 2, 3, 4]\n",
      "Text:        ['My', 'email', 'adress', 'is', 'max.mustermann@web.de']\n",
      "is_alpha:    [True, True, True, True, False]\n",
      "is_punct:    [False, False, False, False, False]\n",
      "like_num:    [False, False, False, False, False]\n",
      "like_email:  [False, False, False, False, True]\n"
     ]
    }
   ],
   "source": [
    "# Lexical attributes\n",
    "doc = nlp(\"My email adress is max.mustermann@web.de\")\n",
    "print(\"Index       \", [token.i for token in doc])\n",
    "print(\"Text:       \", [token.text for token in doc])\n",
    "print(\"is_alpha:   \", [token.is_alpha for token in doc])\n",
    "print(\"is_punct:   \", [token.is_punct for token in doc])\n",
    "print(\"like_num:   \", [token.like_num for token in doc])\n",
    "print(\"like_email: \", [token.like_email for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f777a629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 5.34\n",
      "Percentage found: 3.34\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"The inflation rate is 5.34%. That is 2 percentage points higher than last month (3.34%)\")\n",
    "for token in doc:\n",
    "    if token.like_num:\n",
    "        next_token = doc[token.i + 1]\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Percentage found:\", token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdcd851",
   "metadata": {},
   "source": [
    "## Trained Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44fd7145",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b66697ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "ate VERB\n",
      "the DET\n",
      "pizza NOUN\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "501c2cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj ate\n",
      "ate VERB ROOT ate\n",
      "the DET det pizza\n",
      "pizza NOUN dobj ate\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08e3e61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d75b4d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries, cities, states\n",
      "noun, proper singular\n",
      "Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain(\"GPE\"))\n",
    "print(spacy.explain(\"NNP\"))\n",
    "print(spacy.explain(\"ORG\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2508e0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "determiner\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain(\"det\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f93200fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "direct object\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain(\"dobj\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "951c8437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interjection\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain(\"intj\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e91f6ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjectival modifier\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain(\"amod\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02892a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nominal subject\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain(\"nsubj\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8e49677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modifier of nominal\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain(\"nmod\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4823e59e",
   "metadata": {},
   "source": [
    "## Rule-based matching\n",
    "\n",
    "Why not just use Regex?\n",
    "\n",
    "- match on *Doc* objects , not just strings\n",
    "- match on tokens and token attributes\n",
    "- use a models predictions (e.g. find only \"duck\" only if it is a verb, not a noun)\n",
    "- more flexible: can search texts and other lexical attributes\n",
    "\n",
    "Match patterns\n",
    "\n",
    "- lists of dictionaries (each dictionary describes one token; the dictionary keys are names of token attributes)\n",
    "- e.g. [{\"LEMMA\": \"buy\"}, {\"POS\": \"NOUN\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b03c9b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1b9dc71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.vocab.Vocab at 0x7f3aa5820af0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad0f1854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nuthin\n",
      "ü.\n",
      "p.m\n",
      "Kan\n",
      "Mar\n",
      "When's\n",
      " \n",
      "Sept.\n",
      "c.\n",
      "Mont.\n",
      ":-}\n",
      "12a.m.\n",
      "e.g\n",
      "Why's\n",
      "it\n",
      "6p.m\n",
      "Jr.\n",
      "Who’s\n",
      "K.\n",
      "Calif.\n",
      "e\n",
      "Ill.\n",
      "O'clock\n",
      "o'clock\n",
      "Mich.\n",
      "is\n",
      ":-o\n",
      "n.\n",
      "Might\n",
      "Nov\n",
      ">.<\n",
      "he's\n",
      "it’s\n",
      "where’s\n",
      "Wash.\n",
      "where\n",
      ":-*\n",
      "she's\n",
      "g.\n",
      ":()\n",
      ")-:\n",
      "X\n",
      "S.C.\n",
      "Del\n",
      "Why’s\n",
      "0.o\n",
      "must\n",
      "Goin'\n",
      "4a.m.\n",
      "5p.m.\n",
      "Mass.\n",
      "co.\n",
      "━\n",
      "(-_-)\n",
      "Ariz\n",
      "had\n",
      "0\n",
      "vs.\n",
      "x.\n",
      "><(((*>\n",
      "11a.m.\n",
      "-o\n",
      "When’s\n",
      "Calif\n",
      "does\n",
      "nothin’\n",
      "’S\n",
      "Cos\n",
      "I.e\n",
      "8-)\n",
      "Would\n",
      "do\n",
      "\"\n",
      "Rev.\n",
      "’s\n",
      "N.M.\n",
      "°c.\n",
      "b\n",
      "O.o\n",
      "might\n",
      "q.\n",
      "this's\n",
      "’’\n",
      "Goin’\n",
      "Has\n",
      "N.J.\n",
      "pm\n",
      "ought\n",
      "Dec\n",
      "3p.m\n",
      "<space>\n",
      "Ore.\n",
      "10p.m\n",
      "h.\n",
      "where's\n",
      "doin’\n",
      "What's\n",
      "he’s\n",
      "'cos\n",
      "ä\n",
      "That's\n",
      "11a.m\n",
      "these\n",
      "1p.m\n",
      "Tenn\n",
      ";D\n",
      "Miss\n",
      "Ga\n",
      "Must\n",
      "9p.m\n",
      "somethin\n",
      "Nev.\n",
      "What\n",
      "z.\n",
      "’\n",
      "there's\n",
      "'Cause\n",
      "ü\n",
      "r\n",
      "9\n",
      "Dec.\n",
      "︵\n",
      "v.s\n",
      "Jun.\n",
      ":-))\n",
      ";_;\n",
      "When\n",
      "D.C.\n",
      "Have\n",
      "You\n",
      "10\n",
      "Where's\n",
      "6a.m\n",
      "o_o\n",
      "O\n",
      "'coz\n",
      "'cause\n",
      "co\n",
      "doin\n",
      "how's\n",
      "i.e\n",
      "._.\n",
      "c’m\n",
      "Prof.\n",
      ":’-)\n",
      "havin'\n",
      "Co.\n",
      "Va.\n",
      "Mt\n",
      "|\n",
      ":((\n",
      "O’clock\n",
      "Let\n",
      "Del.\n",
      "xDD\n",
      "Prof\n",
      "i.e.\n",
      "Ai\n",
      "lovin'\n",
      "when’s\n",
      "Does\n",
      "Minn\n",
      "g\n",
      "12a.m\n",
      "Who's\n",
      "-\n",
      "(\n",
      "La\n",
      "11\n",
      "(;\n",
      "12p.m.\n",
      "Mich\n",
      "n’t\n",
      "_\n",
      "Not\n",
      "1p.m.\n",
      "Okla.\n",
      "Okla\n",
      "what's\n",
      "□\n",
      ":P\n",
      "w\n",
      "cos\n",
      ":'-(\n",
      "Ph.D.\n",
      "It’s\n",
      "Va\n",
      "Nothin’\n",
      "Ltd.\n",
      "would\n",
      ":-/\n",
      "Rep\n",
      "Was\n",
      ";\n",
      "a.\n",
      ":)))\n",
      "k\n",
      "o.\n",
      "XD\n",
      "Ga.\n",
      "Sha\n",
      "C\n",
      "Ought\n",
      "Sep\n",
      ">:o\n",
      ":-)\n",
      "<3\n",
      "d\n",
      "8)\n",
      "Who\n",
      "(¬_¬)\n",
      "Pa.\n",
      "f\n",
      "V_V\n",
      ":(((\n",
      "need\n",
      "That’s\n",
      "]\n",
      "-8\n",
      "c\n",
      "(^_^)\n",
      "v.s.\n",
      ":D\n",
      "m\n",
      "cause\n",
      "m.\n",
      "C'm\n",
      "8-\n",
      "0_0\n",
      "Somethin’\n",
      "¬_¬\n",
      "you\n",
      "=\n",
      "that’s\n",
      "was\n",
      "’m\n",
      "How’s\n",
      "F\n",
      "what’s\n",
      "‘s\n",
      "y\n",
      ":o\n",
      "Jr\n",
      "Mont\n",
      ":|\n",
      "Where\n",
      ":1\n",
      "nt\n",
      "this’s\n",
      "0.0\n",
      "11p.m\n",
      "Gen\n",
      "why’s\n",
      "Oct.\n",
      "Let's\n",
      "=D\n",
      "-0\n",
      ".\n",
      "who\n",
      "Gov.\n",
      "May\n",
      "Nov.\n",
      ":o)\n",
      "Mr.\n",
      "’cos\n",
      "'ve\n",
      "Ky.\n",
      "why\n",
      "Are\n",
      "\\\n",
      "how\n",
      "9a.m\n",
      "could\n",
      "5p.m\n",
      "ಠ_ಠ\n",
      "'S\n",
      "Havin’\n",
      "8a.m\n",
      "┻\n",
      "h\n",
      "0_o\n",
      "'m\n",
      "Nuthin'\n",
      "[=\n",
      "ä.\n",
      "2p.m\n",
      "ö.\n",
      "ca\n",
      "'Coz\n",
      "o’clock\n",
      ":p\n",
      "doin'\n",
      "cuz\n",
      ":-(\n",
      ":-((\n",
      "<\n",
      "ol’\n",
      "Nebr.\n",
      "c'm\n",
      "Cuz\n",
      "o.o\n",
      "Havin\n",
      "a.m\n",
      "those\n",
      "space\n",
      "He\n",
      "Neb\n",
      "Ms.\n",
      "Nev\n",
      "9p.m.\n",
      "n't\n",
      "Ol’\n",
      "¯\\(ツ)/¯\n",
      "were\n",
      "t\n",
      "ta\n",
      "nothin'\n",
      "=)\n",
      "a\n",
      "somethin’\n",
      "dare\n",
      "’cause\n",
      "Lovin’\n",
      "’re\n",
      "goin’\n",
      "'Cos\n",
      "Messrs\n",
      "5\n",
      "/3\n",
      "ll\n",
      "this\n",
      "'y\n",
      "*\n",
      "12p.m\n",
      ":x\n",
      "Jan.\n",
      "Sep.\n",
      "Were\n",
      "Jan\n",
      "]=\n",
      "who's\n",
      "Jul.\n",
      "/\n",
      "2a.m.\n",
      "Doin’\n",
      ":(\n",
      "8D\n",
      "j.\n",
      "°k.\n",
      ":-)))\n",
      "nuthin’\n",
      "O_o\n",
      "O_O\n",
      "nuff\n",
      "(-:\n",
      ":\n",
      "they\n",
      "=/\n",
      "He’s\n",
      "^_^\n",
      "We\n",
      "[-:\n",
      "(:\n",
      "’Coz\n",
      "he\n",
      ":-p\n",
      "He's\n",
      "Let’s\n",
      "Can\n",
      "=[\n",
      "'nuff\n",
      "It's\n",
      "Jun\n",
      "'\n",
      "Lovin'\n",
      "l\n",
      "Lovin\n",
      "when's\n",
      "u\n",
      "Ol\n",
      "Dare\n",
      ":-O\n",
      "q\n",
      ";-)\n",
      "there\n",
      "’nuff\n",
      "havin\n",
      "'bout\n",
      "’Cause\n",
      "Need\n",
      "Somethin\n",
      "gon\n",
      "333\n",
      "N.C.\n",
      "\\n\n",
      "E.G.\n",
      "Ia\n",
      "F.\n",
      "b.\n",
      "v\n",
      "got\n",
      "What’s\n",
      "=3\n",
      ">.>\n",
      ":}\n",
      "Wash\n",
      "3a.m\n",
      "Conn\n",
      "w/o\n",
      "Mass\n",
      "Colo\n",
      ":))\n",
      "1a.m.\n",
      "2p.m.\n",
      "11p.m.\n",
      "It\n",
      "j\n",
      "Those\n",
      "bout\n",
      "Ark.\n",
      "who’s\n",
      "that's\n",
      "Ma’am\n",
      "\\\")\n",
      "’coz\n",
      "</3\n",
      "7a.m.\n",
      "Id.\n",
      "St.\n",
      "Mr\n",
      "=|\n",
      "^___^\n",
      "v_v\n",
      ":’-(\n",
      "Bros\n",
      "(>_<)\n",
      "7a.m\n",
      "Mo\n",
      "D.\n",
      ":'(\n",
      "Ma'am\n",
      "Havin'\n",
      "-O\n",
      "Why\n",
      "(=\n",
      "k.\n",
      "K\n",
      "she’s\n",
      "a.m.\n",
      "Wo\n",
      ":-3\n",
      "Apr.\n",
      "Miss.\n",
      "or\n",
      "when\n",
      "o_0\n",
      "Adm.\n",
      "it's\n",
      "Dr\n",
      "Gen.\n",
      "Is\n",
      "lovin’\n",
      "she\n",
      " \n",
      "Feb.\n",
      "Fla.\n",
      "Id\n",
      "-__-\n",
      "Apr\n",
      "Messrs.\n",
      "ol'\n",
      "<333\n",
      "(-;\n",
      "1a.m\n",
      "may\n",
      "(o:\n",
      "=]\n",
      "C’m\n",
      "all\n",
      "Ltd\n",
      "Doin\n",
      "v.v\n",
      "Fla\n",
      "what\n",
      "Kan.\n",
      "-P\n",
      "Minn.\n",
      "p.m.\n",
      "7\n",
      "and/or\n",
      "Ind\n",
      "Sept\n",
      "‘S\n",
      ":X\n",
      "'s\n",
      "6a.m.\n",
      "N.H.\n",
      "''\n",
      "Sen\n",
      "coz\n",
      "i.\n",
      "nothin\n",
      "Bros.\n",
      "8-D\n",
      ":-D\n",
      "[\n",
      "9a.m.\n",
      "Ark\n",
      "This’s\n",
      "’Cuz\n",
      "8p.m\n",
      "Co\n",
      "10a.m.\n",
      "-x\n",
      "This\n",
      "Rev\n",
      "Ms\n",
      "re\n",
      "Gon\n",
      "3a.m.\n",
      "There\n",
      "\\t\n",
      "D\n",
      "Ala.\n",
      "12\n",
      "lovin\n",
      "1\n",
      "S\n",
      "There's\n",
      "4p.m\n",
      "Mrs\n",
      "Ol'\n",
      "(._.)\n",
      "Gov\n",
      ":-0\n",
      "Feb\n",
      "N.Y.\n",
      "-p\n",
      "Nuthin\n",
      "nuthin'\n",
      "ma’am\n",
      "):\n",
      "N.D.\n",
      "Corp\n",
      ")\n",
      "These\n",
      "6p.m.\n",
      "ma'am\n",
      "why's\n",
      "-|\n",
      "Ky\n",
      "C++\n",
      "somethin'\n",
      "e.g.\n",
      "-D\n",
      "not\n",
      "Could\n",
      "That\n",
      "Tenn.\n",
      "did\n",
      "(-8\n",
      "w.\n",
      "’Cos\n",
      "I\n",
      "f.\n",
      ":]\n",
      "I.e.\n",
      "°K.\n",
      "’d\n",
      "-3\n",
      "e.\n",
      "Do\n",
      "V.V\n",
      "Wis\n",
      "Ariz.\n",
      "has\n",
      "Where’s\n",
      "‘\n",
      "’cuz\n",
      "Inc\n",
      "2a.m\n",
      "Md.\n",
      "There’s\n",
      "Ala\n",
      "Ind.\n",
      "°F.\n",
      "Rep.\n",
      ":')\n",
      ";-D\n",
      "have\n",
      "°f.\n",
      "Doin'\n",
      ":>\n",
      "Conn.\n",
      ">\n",
      "She\n",
      "sha\n",
      "y’\n",
      "d.\n",
      "5a.m\n",
      ":O\n",
      "o.O\n",
      "vs\n",
      "╯\n",
      "Did\n",
      ":0\n",
      "7p.m\n",
      "s\n",
      "Nothin'\n",
      "p.\n",
      "-/\n",
      "there’s\n",
      "^__^\n",
      "p\n",
      "'ll\n",
      "She’s\n",
      "'re\n",
      "am\n",
      "'cuz\n",
      "Mo.\n",
      "Corp.\n",
      "Neb.\n",
      "havin’\n",
      "ol\n",
      ">:(\n",
      "<.<\n",
      "Inc.\n",
      "Pa\n",
      "3p.m.\n",
      "°\n",
      ":’(\n",
      ":’)\n",
      "can\n",
      "Jul\n",
      "8\n",
      "Ak.\n",
      "we\n",
      "<33\n",
      "Mt.\n",
      "o_O\n",
      "n\n",
      "em\n",
      "’bout\n",
      "Aug.\n",
      "}\n",
      "(*_*)\n",
      "(╯°□°）╯︵┻━┻\n",
      "on\n",
      "10a.m\n",
      "3\n",
      "Ph\n",
      "How's\n",
      ":/\n",
      "x\n",
      "’ve\n",
      "[:\n",
      "'Cuz\n",
      "Ca\n",
      "that\n",
      "o\n",
      "This's\n",
      ":'-)\n",
      "5a.m.\n",
      "\t\n",
      "y.\n",
      "St\n",
      "Should\n",
      "6\n",
      "@_@\n",
      "y'\n",
      ":3\n",
      "Mrs.\n",
      "10p.m.\n",
      "Oct\n",
      "7p.m.\n",
      "Md\n",
      "ಠ︵ಠ\n",
      "33\n",
      ";)\n",
      "'d\n",
      "Kans\n",
      ":-(((\n",
      "na\n",
      "i\n",
      "-X\n",
      "xD\n",
      "’ll\n",
      "t.\n",
      "°C.\n",
      "Ak\n",
      "8p.m.\n",
      "'em\n",
      "La.\n",
      ":-X\n",
      "v.\n",
      "Nothin\n",
      "Nuthin’\n",
      ":)\n",
      "z\n",
      "-_-\n",
      "(ಠ_ಠ)\n",
      "’em\n",
      "Ia.\n",
      "Sen.\n",
      ":->\n",
      "How\n",
      "4\n",
      "They\n",
      "4p.m.\n",
      "）\n",
      "how’s\n",
      "ö\n",
      "Dr.\n",
      "2\n",
      "ai\n",
      "let’s\n",
      "Had\n",
      "let's\n",
      "8a.m.\n",
      "goin'\n",
      "o.0\n",
      "r.\n",
      "Got\n",
      "Aug\n",
      "l.\n",
      "XDD\n",
      "P\n",
      "Kans.\n",
      "Cause\n",
      "s.\n",
      "wo\n",
      "Goin\n",
      "C.\n",
      ":-x\n",
      "4a.m\n",
      "u.\n",
      "should\n",
      ":-]\n",
      "E.g\n",
      "’y\n",
      "Ill\n",
      "I.E.\n",
      "=(\n",
      "Wis.\n",
      "are\n",
      "Coz\n",
      "Nebr\n",
      "Adm\n",
      "O.O\n",
      "and\n",
      "Colo.\n",
      "Somethin'\n",
      ":-P\n",
      "Ore\n",
      "—\n",
      "goin\n",
      "\n",
      "\n",
      "let\n",
      "Mar.\n",
      "She's\n",
      "ve\n",
      "E.g.\n",
      ":-|\n",
      ":*\n"
     ]
    }
   ],
   "source": [
    "for voc in matcher.vocab:\n",
    "    print(voc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "908c4497",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "matcher.add(\"IPHONE_PATTERN\", [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d355986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Upcoming iPhone X release date leaked. Iphone x\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "43aaa263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9528407286733565721, 1, 3), (9528407286733565721, 7, 9)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ff2e6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n",
      "Iphone x\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c7db3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 FIFA World Cup:\n"
     ]
    }
   ],
   "source": [
    "# matching lexical attributes\n",
    "pattern = [\n",
    "    {\"IS_DIGIT\": True},\n",
    "    {\"LOWER\": \"fifa\"},\n",
    "    {\"LOWER\": \"world\"},\n",
    "    {\"LOWER\": \"cup\"},\n",
    "    {\"IS_PUNCT\": True}\n",
    "]\n",
    "matcher.add(\"FIFA_PATTERN\", [pattern])\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d1df368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loved dogs\n",
      "love cats\n"
     ]
    }
   ],
   "source": [
    "pattern = [\n",
    "    {\"LEMMA\": \"love\", \"POS\": \"VERB\"},\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
    "matcher.add(\"ANIMAL_LOVE_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    print(doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87204598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'possessive ending'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"POS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1b352c",
   "metadata": {},
   "source": [
    "## Operators and Quantifiers\n",
    "\n",
    "{\"OP\": \"!\"} \tNegation: match 0 times \n",
    "\n",
    "{\"OP\": \"?\"} \tOptional: match 0 or 1 times\n",
    "\n",
    "{\"OP\": \"+\"} \tMatch 1 or more times\n",
    "\n",
    "{\"OP\": \"*\"} \tMatch 0 or more times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19d87844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bought a smartphone\n",
      "buying apps\n"
     ]
    }
   ],
   "source": [
    "pattern = [\n",
    "    {\"LEMMA\": \"buy\"},\n",
    "    {\"POS\": \"DET\", \"OP\": \"?\"},  # optional: match 0 or 1 times\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")\n",
    "matcher.add(\"ANIMAL_LOVE_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    print(doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598f2545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
